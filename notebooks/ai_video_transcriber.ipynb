{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "382a12d7",
   "metadata": {},
   "source": [
    "# AI Video Transcriber and Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97b661",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-OpenAI-API-KEY\" data-toc-modified-id=\"1.-OpenAI-API-KEY-1\">1. OpenAI API KEY</a></span></li><li><span><a href=\"#2.-Testing-GPT4-from-LangChain\" data-toc-modified-id=\"2.-Testing-GPT4-from-LangChain-2\">2. Testing GPT4 from LangChain</a></span></li><li><span><a href=\"#3.-Text-Extraction-from-YouTube-with-Whisper\" data-toc-modified-id=\"3.-Text-Extraction-from-YouTube-with-Whisper-3\">3. Text Extraction from YouTube with Whisper</a></span></li><li><span><a href=\"#4.-Prompt-template\" data-toc-modified-id=\"4.-Prompt-template-4\">4. Prompt template</a></span></li><li><span><a href=\"#5.-Chain\" data-toc-modified-id=\"5.-Chain-5\">5. Chain</a></span></li><li><span><a href=\"#6.-Code-Summary\" data-toc-modified-id=\"6.-Code-Summary-6\">6. Code Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e2771",
   "metadata": {},
   "source": [
    "## 1. OpenAI API KEY\n",
    "To carry out this project, we will need an API KEY from OpenAI to use the GPT-4 Turbo model. This API KEY can be obtained at https://platform.openai.com/api-keys. It is only displayed once, so it must be saved at the moment it is obtained. Of course, we will need to create an account to get it.\n",
    "\n",
    "We store the API KEY in a `.env` file to load it with the dotenv library and use it as an environment variable. This file is added to the `.gitignore` to ensure that it cannot be seen if we upload the code to GitHub, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53c8c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API KEY\n",
    "\n",
    "import os                           # operating system library\n",
    "from dotenv import load_dotenv      # load environment variables  \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3817f914",
   "metadata": {},
   "source": [
    "## 2. Testing GPT4 from LangChain\n",
    "We are going to test the connection from LangChain to the GPT-4 model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68324832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Artificial Intelligence (AI) refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. The term can also apply to any machine that exhibits traits associated with a human mind, such as learning and problem-solving. AI systems are used in various fields, from simple tasks like filtering spam emails to more complex ones like self-driving cars, voice recognition systems (like Siri and Alexa), and recommending what you should buy next online.\\n\\nAI can be categorized into several types, including:\\n\\n1. **Narrow AI**: Also known as Weak AI, this kind of artificial intelligence operates within a limited context and is a simulation of human intelligence. Narrow AI is task-specific — examples include Apple's Siri, Amazon's Alexa, and autonomous vehicles. These systems can only perform specific tasks they are programmed for and cannot handle tasks beyond their limited domain.\\n\\n2. **General AI**: Also known as Strong AI, this type of AI will be able to perform any intellectual task that a human being can. General AI would be able to reason, solve problems, make judgments under uncertainty, plan, learn, integrate prior knowledge in decision-making, and be cognitively flexible, handling multiple tasks. This type of AI does not yet exist and is considered a long-term goal of some AI research.\\n\\n3. **Superintelligent AI**: This AI would surpass the intelligence and ability of the human brain. Although purely theoretical at this point and often seen as the stuff of science fiction, the development of superintelligent AI could potentially have unforeseen consequences, possibly being a turning point in human history.\\n\\nAI technologies utilize various methodologies, including machine learning (where computers are programmed to learn from data without being explicitly programmed), deep learning, neural networks, and others to achieve functionality. AI applications can range from simple algorithms used in calculators to complex systems that manage logistics, predict behaviors, and analyze vast amounts of real-time data.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   # LangChain connection to OpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "response = model.invoke(\"What is AI?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2a41b6",
   "metadata": {},
   "source": [
    "## 3. Text Extraction from YouTube with Whisper\n",
    "First, we need to import the libraries we are going to use. We will use `pytube` to access the video and then `Whisper`, the speech-to-text model from OpenAI. We will also need to install `ffmpeg` according to our operating system so that our machine can analyze the audio coming from the YouTube video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95c0e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b23b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: Ray Kurzweil & Geoff Hinton Debate the Future of AI (29:31)\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=kCre83853TM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "910f4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytube for video extraction\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74654f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only want the audio from the video\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a03d849c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2.88G/2.88G [08:09<00:00, 6.31MiB/s]\n"
     ]
    }
   ],
   "source": [
    "# loading Whisper model in local, 2.88G\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1252f562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whisper model description\n",
    "\n",
    "whisper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be03844",
   "metadata": {},
   "source": [
    "**How does Whisper work?**\n",
    "\n",
    "As you can see, Whisper is essentially divided into two parts: an AudioEncoder and a TextDecoder. This is what is known as an AutoEncoder. The AudioEncoder recognizes the audio and vectorizes it, and from those vectors, the TextDecoder extracts the text. To give you a step-by-step, it would be something like this:\n",
    "\n",
    "1. Audio Preprocessing: Whisper begins with the preprocessing of the input audio, where the audio quality is adjusted to improve recognition accuracy. This may include normalizing the volume, filtering background noise, and segmenting the audio into more manageable chunks.\n",
    "\n",
    "2. Feature Extraction: The model extracts features from the processed audio. This involves converting the audio signals into a form that the model can understand, such as spectrograms or Mel-frequency cepstral coefficients (MFCC), which are representations of the sound's frequency and amplitude over time.\n",
    "\n",
    "3. Learning Model: Whisper uses a neural network model, trained on a large amount of audio data and corresponding transcriptions. This model learns to identify patterns and correlations between the audio and textual transcriptions.\n",
    "\n",
    "4. Recognition and Translation: During the recognition phase, Whisper converts the audio into text using its neural network. Additionally, it can translate the recognized text into other languages, thanks to its training in multiple languages.\n",
    "\n",
    "5. Post-processing: Finally, the generated text goes through a post-processing stage to correct common errors, adjust punctuation, and improve the coherence of the text.\n",
    "\n",
    "We will now use Whisper to transcribe the audio to a text file. First, we'll create a temporary directory where the audio will be saved, and then we'll save Whisper's output in the `transcription.txt` file. If the file already exists, we are not interested in converting it again, since Whisper takes a few minutes to transcribe all the audio, so we only run the model if the text file does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c68df4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile    # handling temporal files\n",
    "\n",
    "\n",
    "# save path of transcription\n",
    "\n",
    "PATH_TXT = \"../txt/transcription.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f7b532",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# if text file does not exist...\n",
    "if not os.path.exists(PATH_TXT):\n",
    "    \n",
    "    # open a temporary file...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # audio download from YouTube...\n",
    "        audio_file = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # audio to text whisper transcription.\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        \n",
    "        # save text file\n",
    "        with open(PATH_TXT, \"w\") as text_file:\n",
    "            text_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "\n",
    "with open(PATH_TXT, \"r\") as text_file:\n",
    "    \n",
    "    transcription = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782a92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 1000 text characters\n",
    "    \n",
    "transcription[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df48369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº of text characters\n",
    "\n",
    "len(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7056f7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº of words\n",
    "\n",
    "len(transcription.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4b026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 1000 text characters\n",
    "    \n",
    "transcription[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c9350",
   "metadata": {},
   "source": [
    "## 4. Prompt template\n",
    "Prompt templates are predefined recipes for generating instructions for language models.\n",
    "\n",
    "A template can include instructions, context, and specific questions suitable for a given task. LangChain provides tools for creating and working with instruction templates and also strives to create model-agnostic templates to facilitate the reuse of existing templates across different language models. We will use a template for summarize the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b885ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "            Summarize in a list of 10 bullet points the \n",
    "            following context based on the following question:\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e826210",
   "metadata": {},
   "source": [
    "## 5. Chain\n",
    "A \"chain\" refers to a sequence of components or steps that are linked together to perform a specific task or set of tasks related to AI or LLMs operations. LangChain is a library designed to facilitate the building and deploying of language applications by chaining together different components such as models, databases, and custom logic. Each component in the chain handles a specific part of the task, and the output of one component serves as the input for the next, creating a seamless workflow that leverages both AI and traditional software methodologies. A chain effectively acts as a pipeline, where data flows through each component in the chain, being transformed, enhanced, or utilized at each step.\n",
    "\n",
    "In LangChain, the StrOutputParser parses the model's output directly into a string format. We will use this parser when creating the LangChain sequence; it will be an additional link in the chain, allowing us to directly obtain the LLM's response in string format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4dba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11264b2",
   "metadata": {},
   "source": [
    "**Some examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeacfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main topics of the debate?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b59133",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the political implications?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In economic terms, what is the cost?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e20885",
   "metadata": {},
   "source": [
    "## 6. Code Summary\n",
    "Step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81ddbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pytube import YouTube\n",
    "import whisper\n",
    "\n",
    "import os                           \n",
    "from dotenv import load_dotenv       \n",
    "import tempfile   \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=kCre83853TM\"\n",
    "\n",
    "PATH_TXT = \"../txt/transcription.txt\"\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "\n",
    "if not os.path.exists(PATH_TXT):\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:        \n",
    "        audio_file = audio.download(output_path=dir_temporal)\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        with open(PATH_TXT, \"w\") as text_file:\n",
    "            text_file.write(transcription)\n",
    "\n",
    "\n",
    "            \n",
    "with open(PATH_TXT, \"r\") as text_file:  \n",
    "    transcription = text_file.read()\n",
    "    \n",
    "    \n",
    "template = \"\"\"\n",
    "        Summarize in a list of 10 bullet points the \n",
    "        following context based on the following question:\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be398aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In social terms, what is the posible future?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "126px",
    "top": "0px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
