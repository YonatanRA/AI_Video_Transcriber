{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31372973",
   "metadata": {},
   "source": [
    "# AI Video Transcriber and Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6064a",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-OpenAI-API-KEY\" data-toc-modified-id=\"1.-OpenAI-API-KEY-1\">1. OpenAI API KEY</a></span></li><li><span><a href=\"#2.-Testing-GPT4-from-LangChain\" data-toc-modified-id=\"2.-Testing-GPT4-from-LangChain-2\">2. Testing GPT4 from LangChain</a></span></li><li><span><a href=\"#3.-Text-Extraction-from-YouTube-with-Whisper\" data-toc-modified-id=\"3.-Text-Extraction-from-YouTube-with-Whisper-3\">3. Text Extraction from YouTube with Whisper</a></span></li><li><span><a href=\"#4.-Prompt-template\" data-toc-modified-id=\"4.-Prompt-template-4\">4. Prompt template</a></span></li><li><span><a href=\"#5.-Chain\" data-toc-modified-id=\"5.-Chain-5\">5. Chain</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f3189c",
   "metadata": {},
   "source": [
    "## 1. OpenAI API KEY\n",
    "To carry out this project, we will need an API KEY from OpenAI to use the GPT-4 Turbo model. This API KEY can be obtained at https://platform.openai.com/api-keys. It is only displayed once, so it must be saved at the moment it is obtained. Of course, we will need to create an account to get it.\n",
    "\n",
    "We store the API KEY in a `.env` file to load it with the dotenv library and use it as an environment variable. This file is added to the `.gitignore` to ensure that it cannot be seen if we upload the code to GitHub, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8f1250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API KEY\n",
    "\n",
    "import os                           # operating system library\n",
    "from dotenv import load_dotenv      # load environment variables  \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d79bcde",
   "metadata": {},
   "source": [
    "## 2. Testing GPT4 from LangChain\n",
    "We are going to test the connection from LangChain to the GPT-4 model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b157112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. The term can also apply to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\\n\\nThe capabilities of AI are vast and can range from performing simple tasks such as recognizing a familiar face or understanding spoken words, to more complex functions like interpreting complex data, driving cars autonomously, or even assisting in advanced fields like medicine, finance, and law.\\n\\nAI technology can be categorized into two primary types:\\n\\n1. **Narrow AI**: Also known as Weak AI, this type of artificial intelligence operates within a limited context and is a simulation of human intelligence. Narrow AI is typically focused on performing a single task extremely well and while these machines may seem intelligent, they operate under far more constraints and limitations than even the simplest human intelligence. Examples include image recognition software, chatbots, and recommendation systems.\\n\\n2. **General AI**: Also known as Strong AI, this type of artificial intelligence encompasses a machine with the ability to apply intelligence across a broad range of tasks, mimicking human cognitive abilities. General AI has not yet been achieved and remains a controversial and extensively researched topic.\\n\\nAI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data. AI is an interdisciplinary science with multiple approaches, but advancements in machine learning and deep learning are creating a paradigm shift in virtually every sector of the tech industry.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   # LangChain connection to OpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "response = model.invoke(\"What is AI?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085cb96",
   "metadata": {},
   "source": [
    "## 3. Text Extraction from YouTube with Whisper\n",
    "First, we need to import the libraries we are going to use. We will use `pytube` to access the video and then `Whisper`, the speech-to-text model from OpenAI. We will also need to install `ffmpeg` according to our operating system so that our machine can analyze the audio coming from the YouTube video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5597f93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d439fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: Ray Kurzweil & Geoff Hinton Debate the Future of AI (29:31)\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=kCre83853TM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c4b580f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytube for video extraction\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5540e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only want the audio from the video\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6430ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Whisper model in local, 2.88G\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67df5984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whisper model description\n",
    "\n",
    "whisper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e441d57",
   "metadata": {},
   "source": [
    "**How does Whisper work?**\n",
    "\n",
    "As you can see, Whisper is essentially divided into two parts: an AudioEncoder and a TextDecoder. This is what is known as an AutoEncoder. The AudioEncoder recognizes the audio and vectorizes it, and from those vectors, the TextDecoder extracts the text. To give you a step-by-step, it would be something like this:\n",
    "\n",
    "1. Audio Preprocessing: Whisper begins with the preprocessing of the input audio, where the audio quality is adjusted to improve recognition accuracy. This may include normalizing the volume, filtering background noise, and segmenting the audio into more manageable chunks.\n",
    "\n",
    "2. Feature Extraction: The model extracts features from the processed audio. This involves converting the audio signals into a form that the model can understand, such as spectrograms or Mel-frequency cepstral coefficients (MFCC), which are representations of the sound's frequency and amplitude over time.\n",
    "\n",
    "3. Learning Model: Whisper uses a neural network model, trained on a large amount of audio data and corresponding transcriptions. This model learns to identify patterns and correlations between the audio and textual transcriptions.\n",
    "\n",
    "4. Recognition and Translation: During the recognition phase, Whisper converts the audio into text using its neural network. Additionally, it can translate the recognized text into other languages, thanks to its training in multiple languages.\n",
    "\n",
    "5. Post-processing: Finally, the generated text goes through a post-processing stage to correct common errors, adjust punctuation, and improve the coherence of the text.\n",
    "\n",
    "We will now use Whisper to transcribe the audio to a text file. First, we'll create a temporary directory where the audio will be saved, and then we'll save Whisper's output in the `transcription.txt` file. If the file already exists, we are not interested in converting it again, since Whisper takes a few minutes to transcribe all the audio, so we only run the model if the text file does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64f1cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile    # handling temporal files\n",
    "\n",
    "\n",
    "# save path of transcription\n",
    "\n",
    "PATH_TXT = \"../txt/transcription.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8725151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 41min 48s, sys: 35min 24s, total: 2h 17min 12s\n",
      "Wall time: 11min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# if text file does not exist...\n",
    "if not os.path.exists(PATH_TXT):\n",
    "    \n",
    "    # open a temporary file...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # audio download from YouTube...\n",
    "        audio_file = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # audio to text whisper transcription.\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        \n",
    "        # save text file\n",
    "        with open(PATH_TXT, \"w\") as text_file:\n",
    "            text_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd22be5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "\n",
    "with open(PATH_TXT, \"r\") as text_file:\n",
    "    \n",
    "    transcription = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49b20f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Our opinions on almost everything we talked about were pretty much identical. I think we still disagree probably on whether it's a good idea to live forever. Marvin Minsky was my mentor for 50 years, and whenever consciousness came up he would just dismiss it, that's not real, it's not scientific, and I believe he was correct about it not being scientific, but it certainly is real. I think we're mortal, and we're intrinsically mortal. I'm curious, how do you think about this as the greatest threat and the greatest hope? I just think there's huge uncertainty this year, and we ought to be cautious. And open sourcing these big models is not caution. I agree with that, but I will say last time I talked to you, Jeff, our opinions on almost everything we talked about were pretty much identical, both the dangers and the positives. Positive aspects. In the past I disagreed about how soon superintelligence was coming, and now I think we're pretty much agreed. I think we still disagree probably \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 1000 text characters\n",
    "    \n",
    "transcription[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc87535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25732"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nº of text characters\n",
    "\n",
    "len(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "775ff04e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4701"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nº of words\n",
    "\n",
    "len(transcription.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f67fe9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ings and it doesn't cost that much to train a foundation model, maybe $10 million, maybe $100 million. But a small gang of criminals can't do it to fine tune. An open source model is quite easy. You don't need that much resources. Probably you can do it for a million dollars. And that means they're going to be used for terrible things and they're very powerful things. Well, we can also avoid these dangers with intelligence we get from the same models. Yeah. The AI white hat versus black hat approach. Yes, I had this argument with Jan. And Jan's view is the white hats will always have more resources than the bad guys. Of course, Jan thinks Mark Zuckerberg is a good guy. So we don't necessarily agree on that. I'm I just think there's huge uncertainty and we ought to be cautious. And open sourcing these big models is not caution. All right, Jeff and Ray, thank you so much for your guidance, your wisdom. Ladies and gentlemen, let's give it up for Ray Kurzweil and Jeffrey Hinton. Thank you.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last 1000 text characters\n",
    "    \n",
    "transcription[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb47bb33",
   "metadata": {},
   "source": [
    "## 4. Prompt template\n",
    "Prompt templates are predefined recipes for generating instructions for language models.\n",
    "\n",
    "A template can include instructions, context, and specific questions suitable for a given task. LangChain provides tools for creating and working with instruction templates and also strives to create model-agnostic templates to facilitate the reuse of existing templates across different language models. We will use a template for summarize the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58eec6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f61fc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "            Summarize in a list of 10 bullet points the \n",
    "            following context based on the following question:\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c606c8",
   "metadata": {},
   "source": [
    "## 5. Chain\n",
    "A \"chain\" refers to a sequence of components or steps that are linked together to perform a specific task or set of tasks related to AI or LLMs operations. LangChain is a library designed to facilitate the building and deploying of language applications by chaining together different components such as models, databases, and custom logic. Each component in the chain handles a specific part of the task, and the output of one component serves as the input for the next, creating a seamless workflow that leverages both AI and traditional software methodologies. A chain effectively acts as a pipeline, where data flows through each component in the chain, being transformed, enhanced, or utilized at each step.\n",
    "\n",
    "In LangChain, the StrOutputParser parses the model's output directly into a string format. We will use this parser when creating the LangChain sequence; it will be an additional link in the chain, allowing us to directly obtain the LLM's response in string format.\n",
    "\n",
    "RunnablePassthrough allows inputs to pass through unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82840b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abc93357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aeb12aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af62a338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. The concept of living forever and its desirability.',\n",
       " '2. The scientific nature and reality of consciousness, as discussed by Marvin Minsky.',\n",
       " '3. The potential of generative AI to perform tasks that are currently human-specific.',\n",
       " '4. The integration and merging of humans with AI technologies.',\n",
       " '5. The pace of AI development and its implications on human understanding and control.',\n",
       " '6. The role of AI in advancing scientific discovery, particularly in biology and medicine.',\n",
       " '7. The ethical considerations surrounding AI, including its creativity, rights, and sentient capabilities.',\n",
       " '8. The debate on whether large language AI models should be open-sourced.',\n",
       " '9. The potential risks and benefits of superintelligence and its impact on humanity.',\n",
       " '10. The philosophical and practical implications of AI achieving or surpassing human-like consciousness and intelligence.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What are the main topics of the debate?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab1ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the political implications?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e82f248",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In economic terms, what is the cost?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c984e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "126px",
    "top": "0px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
