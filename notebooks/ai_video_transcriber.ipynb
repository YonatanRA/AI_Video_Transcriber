{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983c98e5",
   "metadata": {},
   "source": [
    "# AI Video Transcriber and Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb0656",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-OpenAI-API-KEY\" data-toc-modified-id=\"1.-OpenAI-API-KEY-1\">1. OpenAI API KEY</a></span></li><li><span><a href=\"#2.-Testing-GPT4-from-LangChain\" data-toc-modified-id=\"2.-Testing-GPT4-from-LangChain-2\">2. Testing GPT4 from LangChain</a></span></li><li><span><a href=\"#3.-Text-Extraction-from-YouTube-with-Whisper\" data-toc-modified-id=\"3.-Text-Extraction-from-YouTube-with-Whisper-3\">3. Text Extraction from YouTube with Whisper</a></span></li><li><span><a href=\"#4.-Prompt-template\" data-toc-modified-id=\"4.-Prompt-template-4\">4. Prompt template</a></span></li><li><span><a href=\"#5.-Chain\" data-toc-modified-id=\"5.-Chain-5\">5. Chain</a></span></li><li><span><a href=\"#6.-Code-Summary\" data-toc-modified-id=\"6.-Code-Summary-6\">6. Code Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134adb7e",
   "metadata": {},
   "source": [
    "## 1. OpenAI API KEY\n",
    "To carry out this project, we will need an API KEY from OpenAI to use the GPT-4 Turbo model. This API KEY can be obtained at https://platform.openai.com/api-keys. It is only displayed once, so it must be saved at the moment it is obtained. Of course, we will need to create an account to get it.\n",
    "\n",
    "We store the API KEY in a `.env` file to load it with the dotenv library and use it as an environment variable. This file is added to the `.gitignore` to ensure that it cannot be seen if we upload the code to GitHub, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0195d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API KEY\n",
    "\n",
    "import os                           # operating system library\n",
    "from dotenv import load_dotenv      # load environment variables  \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ba48b",
   "metadata": {},
   "source": [
    "## 2. Testing GPT4 from LangChain\n",
    "We are going to test the connection from LangChain to the GPT-4 model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411cb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   # LangChain connection to OpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "response = model.invoke(\"What is AI?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2879dd",
   "metadata": {},
   "source": [
    "## 3. Text Extraction from YouTube with Whisper\n",
    "First, we need to import the libraries we are going to use. We will use `pytube` to access the video and then `Whisper`, the speech-to-text model from OpenAI. We will also need to install `ffmpeg` according to our operating system so that our machine can analyze the audio coming from the YouTube video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c9a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807c1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: Ray Kurzweil & Geoff Hinton Debate the Future of AI (29:31)\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=kCre83853TM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytube for video extraction\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48200f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want the audio from the video\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e54ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Whisper model in local, 2.88G\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1da3c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whisper model description\n",
    "\n",
    "whisper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2fe612",
   "metadata": {},
   "source": [
    "**How does Whisper work?**\n",
    "\n",
    "As you can see, Whisper is essentially divided into two parts: an AudioEncoder and a TextDecoder. This is what is known as an AutoEncoder. The AudioEncoder recognizes the audio and vectorizes it, and from those vectors, the TextDecoder extracts the text. To give you a step-by-step, it would be something like this:\n",
    "\n",
    "1. Audio Preprocessing: Whisper begins with the preprocessing of the input audio, where the audio quality is adjusted to improve recognition accuracy. This may include normalizing the volume, filtering background noise, and segmenting the audio into more manageable chunks.\n",
    "\n",
    "2. Feature Extraction: The model extracts features from the processed audio. This involves converting the audio signals into a form that the model can understand, such as spectrograms or Mel-frequency cepstral coefficients (MFCC), which are representations of the sound's frequency and amplitude over time.\n",
    "\n",
    "3. Learning Model: Whisper uses a neural network model, trained on a large amount of audio data and corresponding transcriptions. This model learns to identify patterns and correlations between the audio and textual transcriptions.\n",
    "\n",
    "4. Recognition and Translation: During the recognition phase, Whisper converts the audio into text using its neural network. Additionally, it can translate the recognized text into other languages, thanks to its training in multiple languages.\n",
    "\n",
    "5. Post-processing: Finally, the generated text goes through a post-processing stage to correct common errors, adjust punctuation, and improve the coherence of the text.\n",
    "\n",
    "We will now use Whisper to transcribe the audio to a text file. First, we'll create a temporary directory where the audio will be saved, and then we'll save Whisper's output in the `transcription.txt` file. If the file already exists, we are not interested in converting it again, since Whisper takes a few minutes to transcribe all the audio, so we only run the model if the text file does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile    # handling temporal files\n",
    "\n",
    "\n",
    "# save path of transcription\n",
    "\n",
    "PATH_TXT = \"../txt/transcription.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ecdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# if text file does not exist...\n",
    "if not os.path.exists(PATH_TXT):\n",
    "    \n",
    "    # open a temporary file...\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:\n",
    "        \n",
    "        # audio download from YouTube...\n",
    "        audio_file = audio.download(output_path=dir_temporal)\n",
    "        \n",
    "        # audio to text whisper transcription.\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "        \n",
    "        # save text file\n",
    "        with open(PATH_TXT, \"w\") as text_file:\n",
    "            text_file.write(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text file\n",
    "\n",
    "with open(PATH_TXT, \"r\") as text_file:\n",
    "    \n",
    "    transcription = text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 1000 text characters\n",
    "    \n",
    "transcription[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº of text characters\n",
    "\n",
    "len(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d25410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nº of words\n",
    "\n",
    "len(transcription.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a4f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last 1000 text characters\n",
    "    \n",
    "transcription[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c6c0bf",
   "metadata": {},
   "source": [
    "## 4. Prompt template\n",
    "Prompt templates are predefined recipes for generating instructions for language models.\n",
    "\n",
    "A template can include instructions, context, and specific questions suitable for a given task. LangChain provides tools for creating and working with instruction templates and also strives to create model-agnostic templates to facilitate the reuse of existing templates across different language models. We will use a template for summarize the debate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94de5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "            Summarize in a list of 10 bullet points the \n",
    "            following context based on the following question:\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f3a65e",
   "metadata": {},
   "source": [
    "## 5. Chain\n",
    "A \"chain\" refers to a sequence of components or steps that are linked together to perform a specific task or set of tasks related to AI or LLMs operations. LangChain is a library designed to facilitate the building and deploying of language applications by chaining together different components such as models, databases, and custom logic. Each component in the chain handles a specific part of the task, and the output of one component serves as the input for the next, creating a seamless workflow that leverages both AI and traditional software methodologies. A chain effectively acts as a pipeline, where data flows through each component in the chain, being transformed, enhanced, or utilized at each step.\n",
    "\n",
    "In LangChain, the StrOutputParser parses the model's output directly into a string format. We will use this parser when creating the LangChain sequence; it will be an additional link in the chain, allowing us to directly obtain the LLM's response in string format.\n",
    "\n",
    "RunnablePassthrough allows inputs to pass through unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f5ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0153c99",
   "metadata": {},
   "source": [
    "**Some examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fcf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main topics of the debate?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7296f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the political implications?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3cae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In economic terms, what is the cost?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f10ce9",
   "metadata": {},
   "source": [
    "## 6. Code Summary\n",
    "Step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59d0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   \n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from pytube import YouTube\n",
    "import whisper\n",
    "\n",
    "import os                           \n",
    "from dotenv import load_dotenv       \n",
    "import tempfile   \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=kCre83853TM\"\n",
    "\n",
    "PATH_TXT = \"../txt/transcription.txt\"\n",
    "\n",
    "\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v3\")\n",
    "\n",
    "\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "\n",
    "if not os.path.exists(PATH_TXT):\n",
    "    with tempfile.TemporaryDirectory() as dir_temporal:        \n",
    "        audio_file = audio.download(output_path=dir_temporal)\n",
    "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
    "        with open(PATH_TXT, \"w\") as text_file:\n",
    "            text_file.write(transcription)\n",
    "\n",
    "\n",
    "            \n",
    "with open(PATH_TXT, \"r\") as text_file:  \n",
    "    transcription = text_file.read()\n",
    "    \n",
    "    \n",
    "template = \"\"\"\n",
    "        Summarize in a list of 10 bullet points the \n",
    "        following context based on the following question:\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"In social terms, what is the posible future?\"\n",
    "\n",
    "\n",
    "response = chain.invoke({\"context\": transcription, \"question\": query})\n",
    "\n",
    "\n",
    "response.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ab70b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "126px",
    "top": "0px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
