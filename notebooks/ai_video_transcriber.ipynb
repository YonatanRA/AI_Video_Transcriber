{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "234460f4",
   "metadata": {},
   "source": [
    "# AI Video Transcriber and Summarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f842f9",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-OpenAI-API-KEY\" data-toc-modified-id=\"1.-OpenAI-API-KEY-1\">1. OpenAI API KEY</a></span></li><li><span><a href=\"#2.-Testing-GPT4-from-LangChain\" data-toc-modified-id=\"2.-Testing-GPT4-from-LangChain-2\">2. Testing GPT4 from LangChain</a></span></li><li><span><a href=\"#3.-Text-Extraction-from-YouTube-with-Whisper\" data-toc-modified-id=\"3.-Text-Extraction-from-YouTube-with-Whisper-3\">3. Text Extraction from YouTube with Whisper</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d75299",
   "metadata": {},
   "source": [
    "## 1. OpenAI API KEY\n",
    "To carry out this project, we will need an API KEY from OpenAI to use the GPT-4 Turbo model. This API KEY can be obtained at https://platform.openai.com/api-keys. It is only displayed once, so it must be saved at the moment it is obtained. Of course, we will need to create an account to get it.\n",
    "\n",
    "We store the API KEY in a `.env` file to load it with the dotenv library and use it as an environment variable. This file is added to the `.gitignore` to ensure that it cannot be seen if we upload the code to GitHub, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8e4eabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import API KEY\n",
    "\n",
    "import os                           # operating system library\n",
    "from dotenv import load_dotenv      # load environment variables  \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c21a6f7",
   "metadata": {},
   "source": [
    "## 2. Testing GPT4 from LangChain\n",
    "We are going to test the connection from LangChain to the GPT-4 model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa99239d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. The term can also apply to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\\n\\nThe capabilities of AI are vast and can range from performing simple tasks such as recognizing a familiar face or understanding spoken words, to more complex functions like interpreting complex data, driving cars autonomously, or even assisting in advanced fields like medicine, finance, and law.\\n\\nAI technology can be categorized into two primary types:\\n\\n1. **Narrow AI**: Also known as Weak AI, this type of artificial intelligence operates within a limited context and is a simulation of human intelligence. Narrow AI is typically focused on performing a single task extremely well and while these machines may seem intelligent, they operate under far more constraints and limitations than even the simplest human intelligence. Examples include image recognition software, chatbots, and recommendation systems.\\n\\n2. **General AI**: Also known as Strong AI, this type of artificial intelligence encompasses a machine with the ability to apply intelligence across a broad range of tasks, mimicking human cognitive abilities. General AI has not yet been achieved and remains a controversial and extensively researched topic.\\n\\nAI works by combining large amounts of data with fast, iterative processing and intelligent algorithms, allowing the software to learn automatically from patterns or features in the data. AI is an interdisciplinary science with multiple approaches, but advancements in machine learning and deep learning are creating a paradigm shift in virtually every sector of the tech industry.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI   # LangChain connection to OpenAI\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-4-turbo\")\n",
    "\n",
    "response = model.invoke(\"What is AI?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91bc30",
   "metadata": {},
   "source": [
    "## 3. Text Extraction from YouTube with Whisper\n",
    "First, we need to import the libraries we are going to use. We will use `pytube` to access the video and then `Whisper`, the speech-to-text model from OpenAI. We will also need to install `ffmpeg` according to our operating system so that our machine can analyze the audio coming from the YouTube video.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12ef87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d5a2eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url: Ray Kurzweil & Geoff Hinton Debate the Future of AI (29:31)\n",
    "\n",
    "VIDEO_URL = \"https://www.youtube.com/watch?v=kCre83853TM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ed284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytube for video extraction\n",
    "\n",
    "youtube = YouTube(VIDEO_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c599f448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only want the audio from the video\n",
    "\n",
    "audio = youtube.streams.filter(only_audio=True).first()\n",
    "\n",
    "audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f2b749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Whisper model in local, 2.88G\n",
    "\n",
    "whisper_model = whisper.load_model(\"large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37498be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Whisper(\n",
       "  (encoder): AudioEncoder(\n",
       "    (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): TextDecoder(\n",
       "    (token_embedding): Embedding(51866, 1280)\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x ResidualAttentionBlock(\n",
       "        (attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (cross_attn): MultiHeadAttention(\n",
       "          (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (key): Linear(in_features=1280, out_features=1280, bias=False)\n",
       "          (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (out): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (cross_attn_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (0): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "        )\n",
       "        (mlp_ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whisper model description\n",
    "\n",
    "whisper_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7a9cba",
   "metadata": {},
   "source": [
    "**How does Whisper work?**\n",
    "\n",
    "As you can see, Whisper is essentially divided into two parts: an AudioEncoder and a TextDecoder. This is what is known as an AutoEncoder. The AudioEncoder recognizes the audio and vectorizes it, and from those vectors, the TextDecoder extracts the text. To give you a step-by-step, it would be something like this:\n",
    "\n",
    "1. Audio Preprocessing: Whisper begins with the preprocessing of the input audio, where the audio quality is adjusted to improve recognition accuracy. This may include normalizing the volume, filtering background noise, and segmenting the audio into more manageable chunks.\n",
    "\n",
    "2. Feature Extraction: The model extracts features from the processed audio. This involves converting the audio signals into a form that the model can understand, such as spectrograms or Mel-frequency cepstral coefficients (MFCC), which are representations of the sound's frequency and amplitude over time.\n",
    "\n",
    "3. Learning Model: Whisper uses a neural network model, trained on a large amount of audio data and corresponding transcriptions. This model learns to identify patterns and correlations between the audio and textual transcriptions.\n",
    "\n",
    "4. Recognition and Translation: During the recognition phase, Whisper converts the audio into text using its neural network. Additionally, it can translate the recognized text into other languages, thanks to its training in multiple languages.\n",
    "\n",
    "5. Post-processing: Finally, the generated text goes through a post-processing stage to correct common errors, adjust punctuation, and improve the coherence of the text.\n",
    "\n",
    "We will now use Whisper to transcribe the audio to a text file. First, we'll create a temporary directory where the audio will be saved, and then we'll save Whisper's output in the `transcription.txt` file. If the file already exists, we are not interested in converting it again, since Whisper takes a few minutes to transcribe all the audio, so we only run the model if the text file does not exist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86dc67d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile    # handling temporal files\n",
    "\n",
    "\n",
    "# save path of transcription\n",
    "\n",
    "PATH_TXT = \"../txt/transcription.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f44d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857d193f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b5d5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f51ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede16a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ia",
   "language": "python",
   "name": "ia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "598px",
    "left": "126px",
    "top": "0px",
    "width": "302.398px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
