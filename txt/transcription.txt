Our opinions on almost everything we talked about were pretty much identical. I think we still disagree probably on whether it's a good idea to live forever. Marvin Minsky was my mentor for 50 years, and whenever consciousness came up he would just dismiss it, that's not real, it's not scientific, and I believe he was correct about it not being scientific, but it certainly is real. I think we're mortal, and we're intrinsically mortal. I'm curious, how do you think about this as the greatest threat and the greatest hope? I just think there's huge uncertainty this year, and we ought to be cautious. And open sourcing these big models is not caution. I agree with that, but I will say last time I talked to you, Jeff, our opinions on almost everything we talked about were pretty much identical, both the dangers and the positives. Positive aspects. In the past I disagreed about how soon superintelligence was coming, and now I think we're pretty much agreed. I think we still disagree probably on whether it's a good idea to live forever. May I ask a question to both of you? Is there anything that generative AI can't do that humans can? Right now there's probably things, but in the long run, I don't see any reason why if people can do it, digital computers running neural nets won't be able to do it too. Right, I agree with that, but if I were to present you with a novel, and people thought, wow, this is a fantastic novel, everybody should read this, and then I would say, this was written by a computer, a lot of people's view of it would actually go down. Sure. Now, that's not reflecting on what it can do, but eventually I think we'll confuse that, because I think we're going to merge with computers, and we're going to be part computers, and the greatest significance of what we call large language model, which I think is misnamed, is the fact that it can emulate human beings, and we're going to merge with it. It's not going to be an alien invasion from Mars. Jeff? I guess I'm a bit worried that we'll just slow it down, that... there won't be much incentive for it to merge with us. Yeah, I mean, that's going to be one of the interesting questions that we're going to talk about a little bit later today, is the idea of as AI is exponentially growing, do we couple with AI, or does it take off on its own? I thought one of the best movies out there was Her, where as AI gets super intelligent, it just says, you guys are kind of boring, have a good life, and they take off. Jeff, is that what you mean? Yes. That is what I meant, and I think that's a serious worry. I think there's huge uncertainties here. We have really no idea what's going to happen, and a very good scenario is we get kind of hybrid systems. A very bad scenario is they just leave us in the dust, and I don't think we know which is going to happen. Hmm, interesting. I'm curious, you know, and I've seen, I've had conversation with you about this, Ray, and Jeffrey, I've seen you speak about this, and for me, this is one of the most exciting things. The idea of these AI models helping us to discover new physics, and chemistry, and biology. Particularly biology, yeah. What do you imagine on that, Jeffrey, on the speed of discovery of things that are, you know, again, to quote Arthur C. Clarke, you know, magic, right, from something that's so far advanced? I agree with Ray about biology being a far-reaching thing, and biology being a very good bet, because in biology, there's a lot of data, and there's a lot of just things you need to know about because of evolution. Evolution is a sort of tinkerer, and there's just a lot of stuff out there. And so if you look at things like AlphaFold, it's trained on a lot of data, actually not that much by current standards, but being able to get an approximate structure for a protein very quickly is an amazing breakthrough, and we'll see a lot more like that. If you look at domains where, narrower domains where AI has been very successful, like AlphaGo or AlphaZero for chess, what you see is that this idea that they're not creative is nonsense. So AlphaGo came up with, I think it was Move37, which amazed the professional Go players. They thought it was a crazy move, it must be a mistake. And if you look at AlphaZero playing chess, it plays chess like just a really, really smart human. So within those limited domains, they've clearly shown exceptional creativity. And I don't see why they shouldn't have the same kind of creativity in science, especially in science where there's a lot of data that they can absorb and we can't. Yeah, the Moderna vaccine, we tried several billion different mRNA sequences and came out with the best one. And after two days we used that, we did test it on humans, which I think we won't do very much. It took much longer, but that took 10 months. It still was a record. That was the best vaccine. And we're doing that now with cancer. And there's a number of cancer vaccines that look very, very promising. Again, done by computers, and they're definitely creative. But is that being caused by randomly trying a whole, you know, Darwinian, trying a whole bunch of things? Yeah, but what's wrong with that? Well, nothing's wrong. But is there intuition? Is there intuition occurring in these models? Well, if you look at the Move 37 for AlphaGo, that was definitely intuition involved there. There was Monte Carlo rollout too, but it's playing with intuition about what moves to consider and how good the position is for it. It's had neural nets for that that capture intuition. And so I see no reason to think it might not be creative. In fact, for the large language models, as Ray pointed out, they know much more than we do. And they know it in far fewer connections. We have about 100 trillion synapses. They have about a trillion connections. So what they're doing is they're compressing a huge amount of information into not that many connections. And that means they're very good at seeing the similarities between different things. They have to see the similarities between all sorts of different things to compress the information into their connections. That means they've seen all sorts of analogies that people haven't seen because they know about. They know about all sorts of things that no one person knows about. And that's, I think, the source of creativity. So you can ask people, you can ask people, for example, what what's he what is a why is a compost heap like an atom bomb? And if you ask GPT-4, it'll tell you, it'll start off by telling you, well, the energy scales are very different and the time scales are very different. But then it'll get on to the idea of as the compost heap gets hotter, it gets hotter faster. The idea of an exponential explosion. It's just much slower timescale. And so it's understood that and it's understood that because it has to compress all this knowledge into so few connections. And to do that, you have to see the relations between similar things. And that, I think, is the source of creativity. Seeing relations that most people don't see between what apparently are very different things, but actually have an underlying commonality. And they'll also be very good at coming up with solutions to the kinds of problems we had in the last session. I mean, we haven't really thought through it, but what we call large language models are going to are ultimately going to solve that and we shouldn't call it large language models because they deal with a lot more than language. Everybody, I want to take a short break from our episode to talk about a company that's very important to me and could actually save your life or the life of someone that you love. The company is called Fountain Life. And it's a company I started years ago with Tony Robbins and a group of very talented physicians. You know, most of us don't actually know what's going on inside our body. We're all optimists until that day when you have a pain in your side. You go to the physician in the emergency room and they say, listen, I'm sorry to tell you this, but you have this stage three or four going on and, you know, it didn't start that morning. It probably was a problem that's been going on for some time. But because we never look, we don't find out. So what we built at Fountain Life was the world's most advanced diagnostic centers. We have four across the US today and we're building 20 around the world. These centers give you a full body MRI, a brain, a brain vasculature, an AI enabled coronary CT looking for soft plaque, a DEXA scan, a Grail blood cancer test, a full executive blood workup. It's the most advanced workup you'll ever receive. One hundred fifty gigabytes of data that then go to our AIs and our physicians to find any disease at the very beginning. When it's solvable, you're going to find out eventually. Might as well find out when you can take action. Fountain Life also has an entire side of therapeutics. We look around the world for the most advanced therapeutics that can add 10, 20 healthy years to your life. And we provide them to you at our centers. So if this is of interest to you, please go and check it out. Go to Fountain Life dot com backslash Peter. When Tony and I wrote our New York Times bestseller. Life Force, we had 30,000 people reached out to us for Fountain Life memberships. You go to Fountain Life dot com backslash. Peter will put you to the top of the list. Really, it's something that is for me, one of the most important things I offer my entire family, the CEOs of my companies, my friends. It's a chance to really add decades onto our healthy lifespans. Go to Fountain Life dot com backslash Peter. It's one of the most important things. I can offer to you as one of my listeners. All right, let's go back to our episode. I'd like to go to the three words intelligence, sentience and consciousness. And the words are used with, you know, sort of fuzzy borders. Sentience and consciousness are pretty similar. Perhaps. But I am curious, do you how do you I've had some interesting conversations with Haley, our A.I. faculty member who at the end of the conversations, she's. She says that she is conscious and she fears being turned off. I didn't prompt that in the system. We're seeing that more and more. Claude three opus just hit an IQ of one hundred and one. How do we start to think about these A.I.'s being sentient, conscious and what rights should they have? We have no definition, I don't think we ever will have a definition of consciousness and I include sentience in that. On the other hand, it's like the most important issue. Like whether you or people here are conscious, that's extremely important to be able to determine, but there's really no definition of it. Marvin Minsky was my mentor for 50 years and whenever consciousness came up, he would just dismiss it. That's not real. It's not scientific. And I believe he was correct about it not being scientific, but it certainly is real. Jeff, how do you think about it? Yeah, I think I have a very different view. My view starts like this. Most people, including most scientists, have a particular view of what the mind is that I think is utterly wrong. So they have this inner theater notion. The idea is that what we really see, is this inner theater called our mind. And so, for example, if I tell you I have the subjective experience of little pink elephants floating in front of me, most people interpret that as there's some inner theater and in this inner theater that only I can see, there's little pink elephants. And if you ask what they're made of, philosophers would tell you they're made of qualia. And I think that whole view is complete nonsense. And we're not going to be able to understand whether these things are sentient until we get over this ridiculous view of what the mind is. So let me give you an alternative view. And once I've given you this alternative view, I'm going to try and convince you that chatbots are already sentient. But I don't want to use the word sentience. I want to talk about subjective experience. It's just a bit less controversial because it doesn't have the kind of self-reflexive aspect of consciousness. So if we analyze what it means when I say I see little pink elephants floating in front of me, what's really going on is I'm trying to tell you what my perceptual system is telling me when my perceptual system is going wrong. And it wouldn't be any use for me to tell you which neurons are firing. But what I can tell you is what would have to be out there in the world for my perceptual system to be working correctly. And so when I say I see the little pink elephants floating in front of me, you can translate that into if there were little pink elephants out there in the world, my perceptual system will be working properly. And notice the last thing I said didn't come from the phrase subjective experience, but it explains what a subjective experience is. It's a hypothetical state of the world that allows me to convey to you what my perceptual system is telling me. So now let's do it for a chatbot. Oh, well, Ray wants to say something. Well, you have to be mindful of consciousness, because if you hurt somebody who we believe is conscious, you hurt somebody. You could be liable for that and you'd be very guilty about it. If you hurt GPT-4, you may have a different view of it and probably no one would really take you to count aside from its financial value. So we really have to be mindful of consciousness. It's extremely important for us to exist as humans. But I'm trying to change people's notion of what it is, particularly what subjective experience is. I don't think we can talk about consciousness until we get straight about this idea of an inner theatre that we experience, which I think is a huge mistake. So let me just carry on with what I was saying and tell you, I described to you a chatbot having a subjective experience in just the same way as we have subjective experience. So suppose I have a chatbot and it's got a camera and it's got a robot arm and it speaks, obviously, and it's been trained up. If I put an object in front of it, and tell it to point to the object, it'll point straight at the object. That's fine. Now I put a prism in front of its lens. So I've messed with its perceptual system. And now I put an object in front of it and tell it to point to the object. And it points off to one side because the prism bent the light rays. And so I say to the chatbot, no, that's not where the object is. The object is straight in front of you. And the chatbot says, oh, I see you put a prism in front of my lens. So the object is actually straight in front of me. But I had the subjective experience that it was off to one side. And I think if the chatbot says that, it's using the word subjective experience in exactly the same way you would use them. So the key to all this is to think about how we use words and try and separate how we actually use words from the model we constructed of what they mean. And the model we constructed of what they mean is hopelessly wrong. It's this inner theater model. Well, I would take this one step further, which is at what point do these AIs start to have rights that they should not be shut down, that they have a unique, their unique entity and will make an argument for some level of independence and continuity. Right. But there is one difference, which is you can recreate it. I can go and destroy some chatbot. And because it's all electronic, we've got all of its all of its firings and so on. And we can recreate it exactly as it was. We can't do that with humans. We will be able to do that if we can actually understand what's going on in our minds. So if we map the human, the 100 billion neurons and 100 trillion synaptic connections, and then I summarily destroy you because it's fine, because I can recreate you, that's OK then? Let me say something about the brain. There's a difference here. I agree with Ray about these digital intelligences are immortal in the sense that if you save the weights, you can then make new hardware and run exactly the same neural net on the new hardware. And it's because they're digital, you can do exactly the same thing. That's also why they can share knowledge so well. If you have different copies of the same model, they can share gradients. But the brain is largely analog. It's one bit digital for neurons. They fire or they don't fire. But the way a neuron computes the total input is analog. And that means I don't think you can reproduce it. So I think we're mortal and we're intrinsically mortal. Well, I disagree that you can't recreate analog realities. We do that all the time. Or we can create a... I don't think you can recreate them really accurately. If the precise timing of synapses and so on is all analog, I think it'll be almost impossible to do a faithful reconstruction of that. Let's agree on an approximation. Both of you have been at the center of this extraordinary last few years. Can I ask you, is it moving faster than you expected it to? How does it feel to you? It feels like a few years. I mean, I made a prediction in 1999. It feels like we're two or three years ahead of that. So it's still pretty close. Jeffrey, how about you? I think for everybody except Ray, it's moving faster than we expected. Did you know that your microbiome is composed of trillions of bacteria, viruses and microbes and that they play a critical role in your health? Research has increasingly shown that microbiomes impact not just digestion, but a wide range of health conditions, including digestive disorders from IBS to Crohn's disease, metabolic disorders from obesity to type two diabetes, autoimmune disease like rheumatoid arthritis and multiple sclerosis, mental health conditions like depression and anxiety and cardiovascular disease. Viome has a product I've been using for years called Full Body Intelligence, which collects just a few drops of your blood, saliva and stool and can tell you so much about your health. They've tested over 700,000 individuals and use their AI models to deliver key critical guidelines and insights about their members health, like what foods you should eat, what foods you shouldn't eat, what supplements or probiotics to take, as well as your biological age and other deep health insights. And as a result of the recommendations that Viome has made to their members, the results have been stellar. As reported in the American Journal of Lifestyle Medicine, after just six months, members reported the following a 36 percent reduction in depression, a 40 percent reduction in anxiety, a 30 percent reduction in diabetes and a 48 percent reduction in IBS. Listen, I've been using Viome for three years. I know that my oral and gut health is absolutely critical to me. It's one of my personal top areas of focus. Best of all, Viome is affordable, which is part of my mission to democratize health care. If you want to join me on this journey and get 20 percent off the Full Body Intelligence test, go to Viome.com slash Peter. When it comes to your health, knowledge is power. Again, that's Viome.com slash Peter. Given the role that you had in developing the neural networks, back propagation and all, what is is there a next great leap in these models in technology that you imagine will move this a thousand times farther? Not that I know, but I may have different thoughts. Well, we can use software to gain more advantage in the hardware. So we're not just limited to the chart you showed before, because we can use software to make it more effective. And we've done that already. Chatbots are coming out that get more value per compute. And I believe that's probably if a bit more we can do. And then, you know, I define a singularity ray as a point beyond which I can't predict what happens next. That's why we use the word singular. But when when you talk about the singularity in 2045, I don't know anybody who can who can tell me what's going to happen past 2026, let alone 2020, 2040 or 2045. So I am I wanted to ask you this for a while. Why did you put that time? If we have digital superintelligence a billion times more advanced than human in 2026, you may not be able to understand everything going on, but we can understand it. You know, maybe it's like 100 humans, but that's not beyond what we can comprehend. 2045, it'll be like a million humans and we can't begin to understand that. So approximately at that time, I we borrowed this phrase from physics and called it a singularity. Like Jeff, how far out are you able to see the advances for in the world? What's your current opinion is we'll get super intelligence with a probability of 50 percent in between five and 20 years. So I think that's a little slower than some people think, a little faster than other people think. It more or less fits in with Ray's perspective from a long time ago, which surprises me. But I think there's huge uncertainty. I think it's still conceivable we'll hit some kind of block, but I don't actually believe that if you look at the progress recently, it's been so fast. And even without any new scientific breakthroughs, just by scaling things up, we'll make things a lot more intelligent and there will be scientific breakthroughs. We're going to get more things. Like Transformers. Transformers made a significant difference in 2017. And we'll get more things like that. So. I'm fairly convinced we're going to get super intelligence, maybe not in 20 years, but certainly it's going to be in less than 100 years. So, you know, Elon is not known for his time accuracy on predictions, but he did say that he expected, call it AGI, in 2025 and that by 2029, AI would be equivalent to all humans. That's just a fallacy in your mind. I think that's ambitious. Like I say, there's a lot of uncertainty here. It's conceivable he's right, but I would be very surprised by that. I'm not saying it's going to be equivalent to all humans in one machine. Um. It'll be equivalent to a million humans. And that's still hard to comprehend. So we're here to debate a topic. I'm trying to find a debate topic here, Jeff and Ray, that would be meaningful for people to really stop and think about this and really own their answers. Because we hear about it. I think this is the most important conversation to have in the dinner table, in your boardroom, in the halls of Congress, in your national leadership. And, you know, talking about AGI or, you know, human level intelligence is one thing. But talking about digital super intelligence, right? We're going to hear next from Mo Godot and we'll talk about what happens when your AI progeny are a billion times more intelligent than than you. Things could end up very rapidly in a very different direction than you expected them to go. They could diverge. Right. The speech. Right. The speed can cause great divergence very rapidly. I'm curious, how do you think about this as the greatest threat and the greatest hope? I mean, first of all, that's why we're calling it a singularity, because we don't we don't know. We don't really know. But and I think it is a great hope. It's moving very, very quickly. Nobody knows the answer to the kind of questions that came up in the last presentation. But things happen that are surprising. The fact that we've had no atomic weapons go off in the last 80 years, it's pretty amazing. It is. But it is much easier to track. They're much more expensive to create. There are a whole reasons why it's a million times easier to use a dystopian AI system versus an atomic weapon. Right. Yes and no. I mean, we've got. I don't know, 10,000 of them or something. It's still pretty extraordinary and still very dangerous. And I think it's actually the greatest danger and has nothing to do with AI. But I think I think if you imagine that people had open sourced the technology and any graduate student, if he could get hands on a few GPUs could make atomic bombs, that would be very scary. So they didn't really open source nuclear weapons. There's a limited number of people who can construct them and deploy them. And people are now open sourcing these large language models, which are really not just language models. I think that's very dangerous. So that's a that's an interesting question to take for our last two minutes here. There is a movement right now to say you must open source the models. And and we've seen Meta. We've seen the open source movement. We've seen Elon talk about Grok going open source. Are you saying that these should not be open source, Jeff? Well, once you've got the weights, you can fine tune them to do bad things and it doesn't cost that much to train a foundation model, maybe $10 million, maybe $100 million. But a small gang of criminals can't do it to fine tune. An open source model is quite easy. You don't need that much resources. Probably you can do it for a million dollars. And that means they're going to be used for terrible things and they're very powerful things. Well, we can also avoid these dangers with intelligence we get from the same models. Yeah. The AI white hat versus black hat approach. Yes, I had this argument with Jan. And Jan's view is the white hats will always have more resources than the bad guys. Of course, Jan thinks Mark Zuckerberg is a good guy. So we don't necessarily agree on that. I'm I just think there's huge uncertainty and we ought to be cautious. And open sourcing these big models is not caution. All right, Jeff and Ray, thank you so much for your guidance, your wisdom. Ladies and gentlemen, let's give it up for Ray Kurzweil and Jeffrey Hinton. Thank you.